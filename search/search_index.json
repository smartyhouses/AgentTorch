{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Large Population Models    <p> making complexity simple   differentiable learning over millions of autonomous agents </p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Many grand challenges like climate change and pandemics emerge from complex interactions of millions of individual decisions. While LLMs and AI agents excel at individual behavior, they can't model these intricate societal dynamics. Enter Large Population Models LPMs: a new AI paradigm simulating millions of interacting agents simultaneously, capturing collective behaviors at societal scale. It's like scaling up AI agents exponentially to understand the ripple effects of countless decisions.</p> <p>AgentTorch, our open-source platform, makes building and running these massive simulations accessible. It's optimized for GPUs, allowing efficient simulation of entire cities or countries. Think PyTorch, but for large-scale agent-based simulations. AgentTorch LPMs have four design principles:</p> <ul> <li>Scalability: AgentTorch models can simulate country-size populations in   seconds on commodity hardware.</li> <li>Differentiability: AgentTorch models can differentiate through simulations   with stochastic dynamics and conditional interventions, enabling   gradient-based learning.</li> <li>Composition: AgentTorch models can compose with deep neural networks (eg:   LLMs), mechanistic simulators (eg: mitsuba) or other LPMs. This helps describe   agent behavior using LLMs, calibrate simulation parameters and specify   expressive interaction rules.</li> <li>Generalization: AgentTorch helps simulate diverse ecosystems - humans in   geospatial worlds, cells in anatomical worlds, autonomous avatars in digital   worlds.</li> </ul> <p>LPMs are already making real-world impact. They're being used to help immunize millions of people by optimizing vaccine distribution strategies, and to track billions of dollars in global supply chains, improving efficiency and reducing waste. Our long-term goal is to \"re-invent the census\": built entirely in simulation, captured passively and used to protect country-scale populations. Our research is early but actively making an impact - winning awards at AI conferences and being deployed across the world. Learn more about LPMs here.</p> <p>AgentTorch is building the future of decision engines - inside the body, around us and beyond!</p> <p>https://github.com/AgentTorch/AgentTorch/assets/13482350/4c3f9fa9-8bce-4ddb-907c-3ee4d62e7148</p>"},{"location":"#installation","title":"Installation","text":"<p>The easiest way to install AgentTorch (v0.4.0) is from pypi:</p> <pre><code>&gt; pip install agent-torch\n</code></pre> <p>AgentTorch is meant to be used in a Python 3.9 environment. If you have not installed Python 3.9, please do so first from python.org/downloads.</p> <p>Install the most recent version from source using <code>pip</code>:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>Some models require extra dependencies that have to be installed separately. For more information regarding this, as well as the hardware the project has been run on, please see <code>docs/install.md</code>.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The following section depicts the usage of existing models and population data to run simulations on your machine. It also acts as a showcase of the Agent Torch API.</p> <p>A Jupyter Notebook containing the below examples can be found here.</p>"},{"location":"#executing-a-simulation","title":"Executing a Simulation","text":"<pre><code># re-use existing models and population data easily\nfrom agent_torch.models import covid\nfrom agent_torch.populations import astoria\n\n# use the executor to plug-n-play\nfrom agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\n\n# agent_\"torch\" works seamlessly with the pytorch API\nfrom torch.optim import SGD\n\nloader = LoadPopulation(astoria)\nsimulation = Executor(model=covid, pop_loader=loader)\n\nsimulation.init(SGD)\nsimulation.execute()\n</code></pre>"},{"location":"#guides-and-tutorials","title":"Guides and Tutorials","text":""},{"location":"#understanding-the-framework","title":"Understanding the Framework","text":"<p>A detailed explanation of the architecture of the Agent Torch framework can be found here.</p>"},{"location":"#building-simulations-with-the-configuration-api","title":"Building Simulations with the Configuration API","text":"<p>Learn how to create and customize agent-based simulations using AgentTorch's powerful Configuration API. This tutorial walks you through: - Creating agents with custom properties - Defining environment variables and networks - Building simulation substeps with policies and transitions - Best practices for organizing your simulation</p> <p>Get started with the Config API tutorial \u2192</p>"},{"location":"#optimizing-performance-with-vectorized-operations","title":"Optimizing Performance with Vectorized Operations","text":"<p>Learn how to leverage AgentTorch's vectorized operations for high-performance simulations: - Understanding vectorized vs standard operations - Converting standard functions to vectorized implementations - Using batched processing for large populations - Performance optimization techniques</p> <p>Learn about vectorized operations \u2192</p>"},{"location":"#creating-a-model","title":"Creating a Model","text":"<p>A tutorial on how to create a simple predator-prey model can be found in the <code>tutorials/</code> folder.</p>"},{"location":"#prompting-collective-behavior-with-llm-archetypes","title":"Prompting Collective Behavior with LLM Archetypes","text":"<pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.core.llm.backend import LangchainLLM\nfrom agent_torch.populations import NYC\n\nuser_prompt_template = \"Your age is {age} {gender},{unemployment_rate} the number of COVID cases is {covid_cases}.\"\n\n# Using Langchain to build LLM Agents\nagent_profile = \"You are a person living in NYC. Given some info about you and your surroundings, decide your willingness to work. Give answer as a single number between 0 and 1, only.\"\nllm_langchian = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n)\n\n# Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=7)\n\n# Create an object of the Behavior class\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be generated. This should be the name of any of the regions available in the populations folder.\nearning_behavior = Behavior(\n    archetype=archetype.llm(llm=llm_langchian, user_prompt=user_prompt_template), region=NYC\n)\n\nkwargs = {\n    \"month\": \"January\",\n    \"year\": \"2020\",\n    \"covid_cases\": 1200,\n    \"device\": \"cpu\",\n    \"current_memory_dir\": \"/path-to-save-memory\",\n    \"unemployment_rate\": 0.05,\n}\n\noutput = earning_behavior.sample(kwargs)\n</code></pre>"},{"location":"#contributing-to-agent-torch","title":"Contributing to Agent Torch","text":"<p>Thank you for your interest in contributing! You can contribute by reporting and fixing bugs in the framework or models, working on new features for the framework, creating new models, or by writing documentation for the project.</p> <p>Take a look at the contributing guide for instructions on how to setup your environment, make changes to the codebase, and contribute them back to the project.</p>"},{"location":"#impact","title":"Impact","text":"<p>AgentTorch models are being deployed across the globe.</p> <p></p>"},{"location":"architecture/","title":"Framework Architecture","text":"<p>This document details the architecture of the AgentTorch project, explains all the building blocks involved and points to relevant code implementation and examples.</p> <p>A high-level overview of the AgentTorch Python API is provided by the following block diagram:</p> <p></p> <p>The AgentTorch Python API provides developers with the ability to programmatically create and configure LPMs. This functionality is detailed further in the following sections.</p>"},{"location":"architecture/#runtime","title":"Runtime","text":"<p>The AgentTorch runtime is composed of three essential blocks: the configuration, the registry, and the runner.</p> <p>The configuration holds information about the environment, initial and current state, agents, objects, network metadata, as well as substep definitions. The 'configurator' is defined in <code>config.py</code>.</p> <p>The registry stores all registered substeps, and helper functions, to be called by the runner. It is defined in <code>registry.py</code>.</p> <p>The runner accepts a registry and configuration, and exposes an API to execute all, single or multiple episodes/steps in a simulation. It also maintains the state and trajectory of the simulation across these episodes. It is defined in <code>runner.py</code>, and the substep execution and optimization logic is part of <code>controller.py</code>.</p>"},{"location":"architecture/#data","title":"Data","text":"<p>The data layer is composed of any raw, domain-specific data used by the model (such as agent or object initialization data, environment variables, etc.) as well as the files (YAML or Python code) used to configure the model. An example of domain-specific data for a LPM can be found in the <code>models/covid/data</code> folder. The configuration for the same model can be found in <code>config.yaml</code>.</p>"},{"location":"architecture/#base-classes","title":"Base Classes","text":"<p>The base classes of <code>Agent</code>, <code>Object</code> and <code>Substep</code> form the foundation of the simulation. The agents defined in the configuration learn and interact with either their environment, other agents, or objects through substeps. Substeps are executed in the order of their definition in the configuration, and are split into three parts: <code>SubstepObservation</code>, <code>SubstepAction</code> and <code>SubstepTransition</code>.</p> <ul> <li>A <code>SubstepObservation</code> is defined to observe the state, and pick out those   variables that are of use to the current substep.</li> <li>A <code>SubstepAction</code>, sometimes called a <code>SubstepPolicy</code>, decides the course of   action based on the observations made, and then simulates that action.</li> <li>A <code>SubstepTransition</code> outputs the updates to be made to state variables based   on the action taken in the substep.</li> </ul> <p>An example of a substep will all three parts defined can be found here.</p>"},{"location":"architecture/#domain-extended-classes","title":"Domain Extended Classes","text":"<p>These classes are defined by the developer/user configuring the model, in accordance with the domain of the model. For example, in the COVID model, citizens of the populace are defined as <code>Agents</code>, and <code>Transmission</code> and <code>Quarantine</code> as substeps.</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thanks for your interest in contributing to Agent Torch! This guide will show you how to set up your environment and contribute to this library.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You must have the following software installed:</p> <ol> <li><code>git</code> (latest)</li> <li><code>python</code> (&gt;= 3.10)</li> </ol> <p>Once you have installed the above, follow these instructions to <code>fork</code> and <code>clone</code> the repository (<code>AgentTorch/AgentTorch</code>).</p> <p>Once you have forked and cloned the repository, you can pick out an issue you want to fix/implement!</p>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<p>Once you have cloned the repository to your computer (say, in <code>~/Code/AgentTorch</code>) and picked the issue you want to tackle, create a virtual environment, install all dependencies, and create a new branch to hold all your work.</p> <pre><code># create a virtual environment\n&gt; python -m venv .venv/\n\n# set it up\n&gt; . .venv/bin/activate\n&gt; pip install -r development.txt\n&gt; pip install -e .\n\n# set up the pre commit hooks\n&gt; pre-commit install --config pre-commit.yaml\n\n# create a new branch\n&gt; git switch master\n&gt; git switch --create branch-name\n</code></pre> <p>While naming your branch, make sure the name is short and self explanatory.</p> <p>Once you have created a branch, you can start coding!</p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<p>The project is structured as follows. The comments written next to the file/folder give a brief explanation of what purpose the file/folder serves.</p> <pre><code>.\n\u251c\u2500\u2500 agent_torch/\n\u2502  \u251c\u2500\u2500 helpers/ # defines helper functions used to initialize or work with the state of the simulation.\n\u2502  \u251c\u2500\u2500 llm/ # contains all the code related to using llms as agents in the simulation\n\u2502  \u251c\u2500\u2500 __init__.py # exports everything to the world\n\u2502  \u251c\u2500\u2500 config.py # handles reading and processing the simulation's configuration\n\u2502  \u251c\u2500\u2500 controller.py # executes the substeps for each episode\n\u2502  \u251c\u2500\u2500 initializer.py # creates a simulation from a configuration and registry\n\u2502  \u251c\u2500\u2500 registry.py # registry that stores references to the implementations of the substeps and helper functions\n\u2502  \u251c\u2500\u2500 runner.py # executes the episodes of the simulation, and handles its state\n\u2502  \u251c\u2500\u2500 substep.py # contains base classes for the substep observations, actions and transitions\n\u2502  \u2514\u2500\u2500 utils.py # utility functions used throughout the project\n\u251c\u2500\u2500 docs/\n\u2502  \u251c\u2500\u2500 media/ # assets like screenshots or diagrams inserted in .md files\n\u2502  \u251c\u2500\u2500 tutorials/ # jupyter notebooks with tutorials and their explanations\n\u2502  \u251c\u2500\u2500 architecture.md # the framework's architecture\n\u2502  \u2514\u2500\u2500 install.md # instructions on installing the framework\n\u251c\u2500\u2500 models/\n\u2502  \u251c\u2500\u2500 covid/ # a model simulating disease spread, using the example of covid 19\n\u2502  \u2514\u2500\u2500 predator_prey/ # a simple model used to showcase the features of the framework\n\u251c\u2500\u2500 citation.bib # contains the latex code to use to cite this project\n\u251c\u2500\u2500 contributing.md # this file, helps onboard contributors\n\u251c\u2500\u2500 license.md # contains the license for this project (MIT)\n\u251c\u2500\u2500 readme.md # contains details on the what, why, and how\n\u251c\u2500\u2500 requirements.txt # lists the dependencies of the framework\n\u2514\u2500\u2500 setup.py # defines metadata for the project\n</code></pre> <p>Note that after making any code changes, you should run the <code>black</code> code formatter, as follows:</p> <pre><code>&gt; black agent_torch/ tests/\n</code></pre> <p>You should also ensure all the unit tests pass, especially if you have made changes to any files in the <code>agent_torch/</code> folder.</p> <pre><code>&gt; pytest -vvv tests/\n</code></pre> <p>For any changes to the documentation, run <code>prettier</code> over the <code>*.md</code> files after making changes to them. To preview the generated documentation, run:</p> <pre><code>&gt; mkdocs serve\n</code></pre> <p>Rememeber to add any new pages to the sidebar by editing <code>mkdocs.yaml</code>.</p> <p>If you wish to write a tutorial, write it in a Jupyter Notebook, and then convert it to a markdown file using <code>nbconvert</code>:</p> <pre><code>&gt; pip install nbconvert\n&gt; jupyter nbconvert --to markdown &lt;file&gt;.ipynb\n&gt; mv &lt;file&gt;.md index.md\n</code></pre> <p>Rememeber to move any files that it generates to the <code>docs/media</code> folder, and update the hyperlinks in the generated markdown file.</p>"},{"location":"contributing/#saving-changes","title":"Saving Changes","text":"<p>After you have made changes to the code, you will want to <code>commit</code> (basically, Git's version of save) the changes. To commit the changes you have made locally:</p> <pre><code>&gt; git add this/folder that-file.js\n&gt; git commit --message 'commit-message'\n</code></pre> <p>While writing the <code>commit-message</code>, try to follow the below guidelines:</p> <p>Prefix the message with <code>type:</code>, where <code>type</code> is one of the following dependending on what the commit does:</p> <ul> <li><code>fix</code>: Introduces a bug fix.</li> <li><code>feat</code>: Adds a new feature.</li> <li><code>test</code>: Any change related to tests.</li> <li><code>perf</code>: Any performance related change.</li> <li><code>meta</code>: Any change related to the build process, workflows, issue templates,   etc.</li> <li><code>refc</code>: Any refactoring work.</li> <li><code>docs</code>: Any documentation related changes.</li> </ul> <p>Try to keep the first line brief, and less than 60 characters. Describe the change in detail in a new paragraph (double newline after the first line).</p>"},{"location":"contributing/#contributing-changes","title":"Contributing Changes","text":"<p>Once you have committed your changes, you will want to <code>push</code> (basically, publish your changes to GitHub) your commits. To push your changes to your fork:</p> <pre><code>&gt; git push origin branch-name\n</code></pre> <p>If there are changes made to the <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository, you may wish to merge those changes into your branch. To do so, you can run the following commands:</p> <pre><code>&gt; git fetch upstream master\n&gt; git merge upstream/master\n</code></pre> <p>This will automatically add the changes from <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository to the current branch. If you encounter any merge conflicts, follow this guide to resolve them.</p> <p>Once you have pushed your changes to your fork, follow these instructions to open a <code>pull request</code>:</p> <p>Once you have submitted a pull request, the maintainers of the repository will review your pull requests and provide feedback. If they find the work to be satisfactory, they will merge the pull request.</p>"},{"location":"contributing/#thanks-for-contributing","title":"Thanks for contributing!","text":""},{"location":"install/","title":"Installation Guide","text":"<p>AgentTorch is meant to be used in a Python 3.9 environment (or above). If you have not installed Python 3.9, please do so first from python.org/downloads.</p> <p>To install the project, run:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>To run some models, you may need to separately install their dependencies. These usually include <code>torch</code>, <code>torch_geometric</code>, and <code>osmnx</code>.</p> <p>For the sake of completeness, a summary of the commands required is given below:</p> <pre><code># on macos, cuda is not available:\n&gt; pip install torch torchvision torchaudio\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv\n&gt; pip install osmnx\n\n# on ubuntu, where ${CUDA} is the cuda version:\n&gt; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/${CUDA}\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+${CUDA}.html\n&gt; pip install osmnx\n</code></pre>"},{"location":"install/#hardware","title":"Hardware","text":"<p>The code has been tested on macOS Catalina 10.1.7 and Ubuntu 22.04.2 LTS. Large-scale experiments are run using Nvidia's TitanX and V100 GPUs.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>The following tutorials, in alphabetical order, can be found in this folder:</p> <ul> <li>Creating a simulation</li> <li>Building simulations with Config API</li> <li>Gradient-based calibration</li> <li>Inserting a population</li> <li>From simulation to reality</li> <li>Prompting LLM as LPM Agents</li> </ul>"},{"location":"tutorials/calibrating-a-model/","title":"Calibrating an AgentTorch Model","text":"<p>This tutorial demonstrates how to calibrate parameters in an AgentTorch model using different optimization approaches. We'll explore three methods for parameter optimization and discuss when to use each approach.</p>"},{"location":"tutorials/calibrating-a-model/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of PyTorch and gradient-based optimization</li> <li>Familiarity with AgentTorch's basic concepts</li> <li>Python 3.8+</li> <li>Required packages listed in <code>requirements.txt</code></li> </ul>"},{"location":"tutorials/calibrating-a-model/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#overview","title":"Overview","text":"<p>Model calibration is a crucial step in agent-based modeling. AgentTorch provides several approaches to optimize model parameters:</p> <ol> <li>Internal parameter optimization</li> <li>External parameter optimization</li> <li>Generator-based parameter optimization</li> </ol>"},{"location":"tutorials/calibrating-a-model/#basic-setup","title":"Basic Setup","text":"<p>First, let's set up our environment and import the necessary modules:</p> <pre><code>import warnings\nwarnings.simplefilter(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom agent_torch.models import covid\nfrom agent_torch.populations import sample\nfrom agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\n\n# Initialize simulation\nsim = Executor(covid, pop_loader=LoadPopulation(sample))\nrunner = sim.runner\nrunner.init()\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#helper-classes-and-functions","title":"Helper Classes and Functions","text":"<p>We'll define some helper components that we'll use throughout the tutorial:</p> <pre><code>class LearnableParams(nn.Module):\n    \"\"\"A neural network module that generates bounded parameters\"\"\"\n    def __init__(self, num_params, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.num_params = num_params\n        self.learnable_params = nn.Parameter(torch.rand(num_params, device=self.device))\n        self.min_values = torch.tensor(2.0, device=self.device)\n        self.max_values = torch.tensor(3.5, device=self.device)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self):\n        out = self.learnable_params\n        # Bound output between min_values and max_values\n        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n        return out\n\ndef execute(runner, n_steps=5):\n    \"\"\"Execute simulation and compute loss\"\"\"\n    runner.step(n_steps)\n    labels = runner.state_trajectory[-1][-1]['environment']['daily_infected']\n    return labels.sum()\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#method-1-internal-parameter-optimization","title":"Method 1: Internal Parameter Optimization","text":"<p>The first approach optimizes parameters that are internal to the simulation. This is useful when you want to directly optimize parameters that are part of your model's structure.</p> <pre><code>def optimize_internal_params():\n    # Execute simulation and compute gradients\n    loss = execute(runner)\n    loss.backward()\n\n    # Get gradients of learnable parameters\n    learn_params_grad = [(name, param, param.grad) \n                        for (name, param) in runner.named_parameters()]\n    return learn_params_grad\n\n# Example usage\ngradients = optimize_internal_params()\nprint(\"Internal parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method","title":"When to use this method?","text":"<ul> <li>When parameters are naturally part of your simulation structure</li> <li>When you want direct control over parameter optimization</li> <li>For simpler models with fewer parameters</li> </ul>"},{"location":"tutorials/calibrating-a-model/#method-2-external-parameter-optimization","title":"Method 2: External Parameter Optimization","text":"<p>The second approach involves optimizing external parameters that are fed into the simulation. This provides more flexibility in parameter management.</p> <pre><code>def optimize_external_params():\n    # Create external parameters\n    external_params = nn.Parameter(\n        torch.tensor([2.7, 3.8, 4.6], requires_grad=True)[:, None]\n    )\n\n    # Set parameters in the runner\n    learnable_params = runner.named_parameters()\n    params_dict = {next(iter(learnable_params))[0]: external_params}\n    runner._set_parameters(params_dict)\n\n    # Execute and compute gradients\n    loss = execute(runner)\n    loss.backward()\n    return external_params.grad\n\n# Example usage\ngradients = optimize_external_params()\nprint(\"External parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method_1","title":"When to use this method?","text":"<ul> <li>When you want to manage parameters outside the simulation</li> <li>For parameter sweeps or sensitivity analysis</li> <li>When parameters need to be shared across different components</li> </ul>"},{"location":"tutorials/calibrating-a-model/#method-3-generator-based-parameter-optimization","title":"Method 3: Generator-Based Parameter Optimization","text":"<p>The third approach uses a generator function to predict optimal parameters. This is particularly useful for complex parameter relationships.</p> <pre><code>def optimize_with_generator():\n    # Create generator model\n    learn_model = LearnableParams(3)\n    params = learn_model()[:, None]\n\n    # Execute and compute gradients\n    loss = execute(runner)\n    loss.backward()\n\n    # Get gradients of generator parameters\n    learn_params_grad = [(param, param.grad) \n                        for (name, param) in learn_model.named_parameters()]\n    return learn_params_grad\n\n# Example usage\ngradients = optimize_with_generator()\nprint(\"Generator parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method_2","title":"When to use this method?","text":"<ul> <li>When parameters have complex relationships</li> <li>For learning parameter patterns</li> <li>When you want to generate parameters based on conditions</li> </ul>"},{"location":"tutorials/calibrating-a-model/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's how to use all three methods in a complete optimization loop:</p> <pre><code>def calibrate_model(method='internal', num_epochs=10):\n    for epoch in range(num_epochs):\n        if method == 'internal':\n            gradients = optimize_internal_params()\n        elif method == 'external':\n            gradients = optimize_external_params()\n        else:  # generator\n            gradients = optimize_with_generator()\n\n        print(f\"Epoch {epoch}, gradients: {gradients}\")\n        # Add your optimizer step here\n\n# Example usage\ncalibrate_model(method='internal', num_epochs=3)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Method: Consider your specific use case when selecting an optimization approach.</li> <li>Monitor Convergence: Always track your loss function to ensure proper optimization.</li> <li>Validate Results: Cross-validate your calibrated parameters with held-out data.</li> <li>Handle Constraints: Use appropriate bounds and constraints for your parameters.</li> </ol>"},{"location":"tutorials/calibrating-a-model/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Ensure parameters have appropriate ranges</li> <li>Watch out for local optima</li> <li>Be careful with learning rates in optimization</li> <li>Consider the computational cost of each approach</li> </ul>"},{"location":"tutorials/calibrating-a-model/#conclusion","title":"Conclusion","text":"<p>We've explored three different approaches to model calibration in AgentTorch. Each method has its strengths and is suited for different scenarios. Choose the approach that best matches your specific needs and model complexity.</p>"},{"location":"tutorials/calibrating-a-model/#additional-resources","title":"Additional Resources","text":"<ul> <li>AgentTorch Documentation</li> <li>PyTorch Optimization</li> <li>Related Tutorials</li> </ul>"},{"location":"tutorials/compare-llm-performance/","title":"Benchmark: Sensitivity to Agent Behaviors","text":""},{"location":"tutorials/compare-llm-performance/#exploring-llm-influence","title":"Exploring LLM Influence","text":""},{"location":"tutorials/compare-llm-performance/#introduction","title":"Introduction","text":"<p>AgentTorch is a framework that scales ABM simulations to real-world problems. This tutorial will guide you through the process of experimenting with different LLMs in AgentTorch to understand their impact on the framework's effectiveness.</p>"},{"location":"tutorials/compare-llm-performance/#step-1-setup","title":"Step 1: Setup","text":"<p>First, let's set up our environment and import the necessary libraries:</p> <pre><code>import sys\nfrom dspy_modules import COT, BasicQAWillToWork\nfrom agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\nfrom agent_torch.core.llm.backend import DspyLLM, LangchainLLM\n\nimport torch\nOPENAI_API_KEY = None\n</code></pre> <p>Setup : Covid Cases Data and Unemployment Rate</p> <pre><code>from utils import get_covid_cases_data\ncsv_path = 'agent_torch/models/covid/data/county_data.csv'\nmonthly_cases_kings = get_covid_cases_data(csv_path=csv_path,county_name='Kings County')\nmonthly_cases_queens = get_covid_cases_data(csv_path=csv_path,county_name='Queens County')\nmonthly_cases_bronx = get_covid_cases_data(csv_path=csv_path,county_name='Bronx County')\nmonthly_cases_new_york = get_covid_cases_data(csv_path=csv_path,county_name='New York County')\nmonthly_cases_richmond = get_covid_cases_data(csv_path=csv_path,county_name='Richmond County')\n\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-2-initialise-llm-instance","title":"Step 2: Initialise LLM Instance","text":"<p>We can use either of the Langchain and Dspy backends to initialise a LLM instance. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch.</p> <p>Let's see how we can use Langchain to initialise an LLM instance</p> <p>GPT 3.5 Turbo</p> <pre><code>agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\nllm_langchain_35 = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n)\n</code></pre> <p>GPT 4-0 Mini</p> <pre><code>agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\nllm_langchain_4o_mini = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-4o-mini\"\n) \n</code></pre> <p>Similarly if we wanted to use Dspy backend, we can instantiate the DspyLLM object. We can pass the desired model name as argument just like we did with Langchain.</p> <pre><code># Agent profile is decided by the QA module and the COT module enforces Chain of Thought reasoning\nllm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-3-define-agent-behavior","title":"Step 3: Define agent Behavior","text":"<p>Create an object of the Behavior class You have options to pass any of the above created llm objects to the behavior class Specify the region for which the behavior is to be generated. This should be the name of any of the regions available in the populations folder.</p> <pre><code># Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=2) \n\n# Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age}, gender is {gender}, ethnicity is {ethnicity}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior_4o_mini = Behavior(\n    archetype=archetype.llm(llm=llm_langchain_4o_mini, user_prompt=user_prompt_template),\n    region=NYC\n)\nearning_behavior_35 = Behavior(\n    archetype=archetype.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n    region=NYC\n)\n</code></pre> <pre><code># Define arguments to be used for creating a query for the LLM instance\nkwargs = {\n    \"month\": \"January\",\n    \"year\": \"2020\",\n    \"covid_cases\": 1200,\n    \"device\": \"cpu\",\n    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n    \"unemployment_rate\": 0.05,\n}\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-4-compare-performance-between-different-llm-models","title":"Step 4: Compare performance between different LLM models","text":"<pre><code>from utils import get_labor_data, get_labor_force_correlation\n\nlabor_force_df_4o_mini, observed_labor_force_4o_mini, correlation_4o_mini = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_4o_mini, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nlabor_force_df_35, observed_labor_force_35, correlation_35 = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_35, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nprint(f\"Correlation with GPT 3.5 is {correlation_35} and with GPT 4o Mini is {correlation_4o_mini}\")\n</code></pre>"},{"location":"tutorials/config_api/","title":"AgentTorch Configuration API Tutorial","text":"<p>This tutorial will guide you through using AgentTorch's Configuration API to set up simulations. We'll use a simple movement simulation as our primary example and then explore advanced features.</p>"},{"location":"tutorials/config_api/#basic-usage-movement-simulation","title":"Basic Usage: Movement Simulation","text":"<p>Let's create a simulation where agents move randomly within a bounded space. The full code is available here. This example demonstrates the core concepts of AgentTorch's configuration system.</p>"},{"location":"tutorials/config_api/#project-structure","title":"Project Structure","text":"<pre><code>agent_torch/examples/models/movement/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 substeps/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 random_move.py\n\u2502   \u2514\u2500\u2500 update_position.py\n\u2514\u2500\u2500 yamls/\n    \u2514\u2500\u2500 config.yaml\n</code></pre>"},{"location":"tutorials/config_api/#setting-up-the-configuration","title":"Setting Up the Configuration","text":"<p>First, import the necessary builders:</p> <pre><code>from agent_torch.config import (\n    ConfigBuilder,\n    StateBuilder,\n    AgentBuilder,\n    PropertyBuilder,\n    EnvironmentBuilder,\n    SubstepBuilder,\n    PolicyBuilder,\n    TransitionBuilder\n)\n</code></pre>"},{"location":"tutorials/config_api/#1-simulation-metadata","title":"1. Simulation Metadata","text":"<p>Define the basic simulation parameters (note that all these are REQUIRED PARAMETERS):</p> <pre><code>metadata = {\n    \"num_agents\": 1000,\n    \"num_episodes\": 10,\n    \"num_steps_per_episode\": 20,\n    \"num_substeps_per_step\": 1,\n    \"device\": \"cpu\",\n    \"calibration\": False # if learnable parameters will be optimized externally [more details in gradient-based calibration tutorial]\n}\nconfig.set_metadata(metadata)\n</code></pre>"},{"location":"tutorials/config_api/#2-state-configuration","title":"2. State Configuration","text":"<p>Configure the simulation state including agents and environment:</p> <pre><code># Build state\nstate_builder = StateBuilder()\n\n# Add agent with position property\nagent_builder = AgentBuilder(\"citizens\", metadata[\"num_agents\"])\nposition = PropertyBuilder(\"position\")\\\n    .set_dtype(\"float\")\\\n    .set_shape([metadata[\"num_agents\"], 2])\\\n    .set_value([0.0, 0.0])\nagent_builder.add_property(position)\nstate_builder.add_agent(\"citizens\", agent_builder)\n\n# Add environment bounds\nenv_builder = EnvironmentBuilder()\nbounds = PropertyBuilder(\"bounds\")\\\n    .set_dtype(\"float\")\\\n    .set_shape([2])\\\n    .set_value([100.0, 100.0])\nenv_builder.add_variable(bounds)\nstate_builder.set_environment(env_builder)\n\n# Set state in config\nconfig.set_state(state_builder.to_dict())\n</code></pre>"},{"location":"tutorials/config_api/#3-substep-configuration","title":"3. Substep Configuration","text":"<p>Define the simulation substeps with their policies and transitions:</p> <pre><code># Create movement substep\nmovement = SubstepBuilder(\"Movement\", \"Agent movement simulation\")\nmovement.add_active_agent(\"citizens\")\nmovement.config[\"observation\"] = {\"citizens\": None}\n\n# Add movement policy\npolicy = PolicyBuilder()\nstep_size = PropertyBuilder.create_argument(\n    name=\"Step size parameter\",\n    value=1.0,\n    learnable=True\n).config\n\npolicy.add_policy(\n    \"move\",\n    \"RandomMove\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"direction\"],\n    {\"step_size\": step_size}\n)\nmovement.set_policy(\"citizens\", policy)\n\n# Add movement transition\ntransition = TransitionBuilder()\nbounds_param = PropertyBuilder.create_argument(\n    name=\"Environment bounds\",\n    value=[100.0, 100.0],\n    shape=[2],\n    learnable=True\n).config\n\ntransition.add_transition(\n    \"update_position\",\n    \"UpdatePosition\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"position\"],\n    {\"bounds\": bounds_param}\n)\nmovement.set_transition(transition)\n\n# Add substep to config and save\nconfig.add_substep(\"0\", movement)\nconfig.save_yaml(\"models/movement/yamls/config.yaml\")\n</code></pre>"},{"location":"tutorials/config_api/#4-manual-substep-implementation","title":"4. Manual Substep Implementation","text":"<p>The substeps can be implemented manually as Python classes. Here's an example of the movement policy:</p> <pre><code>@Registry.register_substep(\"move\", \"policy\")\nclass RandomMove(SubstepAction):\n    def forward(self, state: Dict[str, Any], observations) -&gt; Dict[str, Any]:\n        positions = get_var(state, self.input_variables[\"position\"])\n        num_agents = positions.shape[0]\n\n        # Get step size from learnable arguments\n        self.step_size = self.learnable_args[\"step_size\"]\n\n        # Generate random angles and directions\n        angles = torch.rand(num_agents) * 2 * torch.pi\n        direction = torch.stack([\n            torch.cos(angles) * self.step_size,\n            torch.sin(angles) * self.step_size\n        ], dim=1)\n\n        # Return using output variable name from config\n        outputs = {}\n        outputs[self.output_variables[0]] = direction\n        return outputs\n</code></pre>"},{"location":"tutorials/config_api/#5-setting-up-module-structure","title":"5. Setting Up Module Structure","text":"<p>The substeps need to be properly imported and registered. This is done through <code>__init__.py</code> files:</p> <pre><code># models/movement/substeps/__init__.py\n\"\"\"Substep implementations for movement simulation.\"\"\"\nfrom .random_move import *\nfrom .update_position import *\n</code></pre> <pre><code># models/movement/__init__.py\n\"\"\"Movement simulation model.\"\"\"\nfrom agent_torch.core import Registry\nfrom agent_torch.core.helpers import *\n\nfrom .substeps import *\n\n# Create and populate registry as a module-level variable\nregistry = Registry()\n</code></pre> <p>These files ensure that: 1. All substep implementations are imported when the movement module is imported 2. The registry is created at the module level 3. Substeps are automatically registered via their decorators when imported</p>"},{"location":"tutorials/config_api/#6-running-the-simulation","title":"6. Running the Simulation","text":"<p>The movement simulation can be run using:</p> <pre><code># run_movement_sim.py\nfrom agent_torch.populations import sample2\nfrom agent_torch.examples.models import movement\nfrom agent_torch.core.environment import envs\n\ndef run_movement_simulation():\n    \"\"\"Run the movement simulation.\"\"\"\n    # Create simulation runner\n    runner = envs.create(\n        model=movement,\n        population=sample2\n    )\n\n    # Get simulation parameters\n    sim_steps = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]\n    num_episodes = runner.config[\"simulation_metadata\"][\"num_episodes\"]\n\n    # Run episodes\n    for episode in range(num_episodes):\n        if episode &gt; 0:\n            runner.reset()\n        runner.step(sim_steps)\n\n        # Print statistics\n        positions = runner.state[\"agents\"][\"citizens\"][\"position\"]\n        print(f\"Episode {episode + 1} - Average position: {positions.mean(dim=0)}\")\n\nif __name__ == \"__main__\":\n    runner = run_movement_simulation()\n</code></pre> <p>To run the simulation:</p> <pre><code>python -m agent_torch.examples.run_movement_sim\n</code></pre>"},{"location":"tutorials/config_api/#advanced-usage-automatic-substep-generation","title":"Advanced Usage: Automatic Substep Generation","text":"<p>Instead of writing substeps manually, AgentTorch provides <code>SubstepBuilderWithImpl</code> to automatically generate implementation templates. Let's see how we could have used it for our movement example:</p> <pre><code>from agent_torch.config import SubstepBuilderWithImpl\n\n# Create substep with implementation generation\nmovement_substep = SubstepBuilderWithImpl(\n    name=\"Movement\", \n    description=\"Agent movement simulation\",\n    output_dir=\"models/movement/substeps\"\n)\nmovement_substep.add_active_agent(\"citizens\")\nmovement_substep.config[\"observation\"] = {\"citizens\": None}\n\n# Add same policy and transition configurations\npolicy = PolicyBuilder()\npolicy.add_policy(\n    \"move\",\n    \"RandomMove\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"direction\"],\n    {\"step_size\": step_size}\n)\nmovement_substep.add_policy(\"citizens\", policy)\n\ntransition = TransitionBuilder()\ntransition.add_transition(\n    \"update_position\",\n    \"UpdatePosition\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"position\"],\n    {\"bounds\": bounds_param}\n)\nmovement_substep.set_transition(transition)\n\n# Generate implementation files\ngenerated_files = movement_substep.generate_implementations()\n</code></pre> <p>This will generate template files for both <code>random_move.py</code> and <code>update_position.py</code> with: - Proper imports and class structure - Registry decorators - Input/output variable handling - TODO sections for implementation logic</p> <p>You would then only need to fill in the forward logic in the TODO sections.</p>"},{"location":"tutorials/config_api/#more-examples","title":"More Examples","text":"<p>For a more complex example, check out the COVID-19 simulation in <code>agent_torch/examples/models/covid/</code>. This example demonstrates: - Multiple substeps with complex interactions - Custom observation and reward functions - Network-based agent interactions - Advanced use of learnable parameters</p> <p>You can run it with:</p> <pre><code>python -m agent_torch.examples.run_covid_sim\n</code></pre>"},{"location":"tutorials/config_api/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Metadata: Always include required fields like <code>num_agents</code>, <code>num_episodes</code>, <code>num_steps_per_episode</code>, and <code>num_substeps_per_step</code>.</p> </li> <li> <p>PropertyBuilder Arguments: Use <code>PropertyBuilder.create_argument()</code> for learnable parameters, and access them in substeps via <code>self.learnable_args</code> or <code>self.arguments</code> (if not learnable).</p> </li> <li> <p>Output Variables: Always use <code>self.output_variables[index]</code> as dictionary keys when returning from substep forward methods to ensure consistency with the configuration.</p> </li> <li> <p>Observation Structure: When no observation is needed, use <code>{\"citizens\": None}</code> instead of an empty dict.</p> </li> <li> <p>Implementation Generation: Use <code>SubstepBuilderWithImpl</code> for complex simulations to automatically generate consistent substep templates. This is especially helpful when you have many substeps or want to ensure consistent structure across your codebase. </p> </li> </ol>"},{"location":"tutorials/configure-behavior/","title":"Guide to Prompting LLM as ABM Agents","text":"<p>Welcome to this comprehensive tutorial on AI agent behavior generation! This guide is designed for newcomers who want to learn how to create AI agents and simulate population behaviors using a custom framework. We'll walk you through each step, explaining concepts as we go.</p>"},{"location":"tutorials/configure-behavior/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to the Framework</li> <li>Setting Up Your Environment</li> <li>Understanding the Core Components</li> <li>Creating Your First AI Agent</li> <li>Generating Population Behaviors</li> <li>Putting It All Together</li> <li>Next Steps and Advanced Topics</li> </ol>"},{"location":"tutorials/configure-behavior/#1-introduction-to-the-framework","title":"1. Introduction to the Framework","text":"<p>Our framework is designed to simulate population behaviors using AI agents. It combines several key components:</p> <ul> <li>LLM Agents: We use Large Language Models (LLMs) to create intelligent   agents that can make decisions based on given scenarios.</li> <li>Archetypes: These represent different types of individuals in a   population.</li> <li>Behaviors: These simulate how individuals might act in various situations.</li> </ul> <p>This framework is particularly useful for modeling complex social or economic scenarios, such as population responses during a pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<p>Now, let's set up your OpenAI API key (you'll need an OpenAI account):</p> <pre><code>OPENAI_API_KEY = None # Replace with your actual API key\n</code></pre> <p></p>"},{"location":"tutorials/configure-behavior/#3-understanding-the-core-components","title":"3. Understanding the Core Components","text":"<p>Let's break down the main components of our framework:</p>"},{"location":"tutorials/configure-behavior/#dspyllm-and-langchainllm","title":"DspyLLM and LangchainLLM","text":"<p>These are wrappers around language models that allow us to create AI agents. They can process prompts and generate responses based on given scenarios.</p>"},{"location":"tutorials/configure-behavior/#archetype","title":"Archetype","text":"<p>This component helps create different \"types\" of individuals in our simulated population. Like Male under 19, Female from 20 to 29 years of age.</p>"},{"location":"tutorials/configure-behavior/#behavior","title":"Behavior","text":"<p>The Behavior component simulates how individuals (or groups) might act in various situations. It uses the AI agents to generate these behaviors.</p> <p>Now you have two AI agents ready to process prompts!</p>"},{"location":"tutorials/configure-behavior/#4-creating-llm-agents","title":"4. Creating LLM Agents","text":"<p>We support using Langchain and Dspy backends to initialize LLM instances - for agent and archetypes. Using our LLMBackend class, you can integrate any framework of your choice.</p>"},{"location":"tutorials/configure-behavior/#using-dspy","title":"Using DSPy","text":"<pre><code>from dspy_modules import COT, BasicQAWillToWork\nfrom agent_torch.core.llm.llm import DspyLLM\n\nllm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)\nllm_dspy.initialize_llm()\n\noutput_dspy = llm_dspy.prompt([\"You are an individual living during the COVID-19 pandemic. You need to decide your willingness to work each month and portion of your assets you are willing to spend to meet your consumption demands, based on the current situation of NYC.\"])\nprint(\"DSPy Output:\", output_dspy)\n</code></pre>"},{"location":"tutorials/configure-behavior/#using-langchain","title":"Using Langchain","text":"<pre><code>from agent_torch.core.llm.llm import LangchainLLM\n\nagent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n\nllm_langchian = LangchainLLM(openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\")\nllm_langchian.initialize_llm()\n\noutput_langchain = llm_langchian.prompt([\"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0.0 and 1.0, only.\"])\nprint(\"Langchain Output:\", output_langchain)\n</code></pre>"},{"location":"tutorials/configure-behavior/#5-generating-population-behaviors","title":"5. Generating Population Behaviors","text":"<p>To simulate population behaviors, we'll use the Archetype and Behavior classes:</p> <pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\n\n# Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=7)\n\n# Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age} {gender}, unemployment rate is {unemployment_rate}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior = Behavior(\n    archetype=archetype.llm(llm=llm_dspy, user_prompt=user_prompt_template, num_agents=12),\n    region=NYC\n)\n\nprint(\"Behavior model created successfully!\")\n</code></pre> <p>This sets up a behavior model that can simulate how 12 different agents might behave in NYC during the COVID-19 pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#6-putting-it-all-together","title":"6. Putting It All Together","text":"<p>Now, let's use our behavior model to generate some population behaviors:</p> <pre><code># Define scenario parameters\n# Names of the parameters should match the placeholders in the user_prompt template\nscenario_params = {\n    'month': 'January',\n    'year': '2020',\n    'covid_cases': 1200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.05,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define another scenario parameters\nscenario_params = {\n    'month': 'February',\n    'year': '2020',\n    'covid_cases': 900,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.1,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define yet another scenario parameters\nscenario_params = {\n    'month': 'March',\n    'year': '2020',\n    'covid_cases': 200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.11,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <p>And so on...</p> <p>This will output a set of behaviors for our simulated population based on the given scenario.</p> <p></p>"},{"location":"tutorials/configure-behavior/#7-next-steps-and-advanced-topics","title":"7. Next Steps and Advanced Topics","text":"<p>You've just created your first AI agents and simulated population behaviors. Here are some advanced topics you might want to explore next:</p> <ul> <li>Customizing archetypes for specific populations</li> <li>Creating more complex behavior models</li> </ul>"},{"location":"tutorials/creating-a-model/","title":"Predator-Prey Model","text":"Imports <pre><code># import agent-torch\n\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('../../../agent_torch'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom AgentTorch import Runner, Registry\nfrom AgentTorch.substep import SubstepObservation, SubstepAction, SubstepTransition\nfrom AgentTorch.helpers import get_by_path, read_config, read_from_file, grid_network\n</code></pre> <pre><code># import all external libraries that we need.\n\nimport math\nimport torch\nimport re\nimport random\nimport argparse\nimport numpy as np\nimport torch.nn as nn\nimport networkx as nx\nimport osmnx as ox\nfrom tqdm import trange\n</code></pre> <pre><code># define the helper functions we need.\n\ndef get_var(state, var):\n  \"\"\"\n    Retrieves a value from the current state of the model.\n  \"\"\"\n  return get_by_path(state, re.split('/', var))\n</code></pre> <p>The complete code for this model can be found here. The architecture of the AgentTorch framework, which explains some key concepts, can be found here.</p> <p>This guide walks you through creating a custom predator-prey model using the AgentTorch framework. This model will simulate an ecosystem consisting of predators, prey and grass: predators eat prey, and prey eat grass.</p> <p>The model's parameters, rules and configuration are passed to AgentTorch, which iteratively simulates the model, allowing you to optimize its learnable parameters, while also modelling the simulation in real time. AgentTorch's Python API is based on PyTorch, which enhances its performance on GPUs.</p> <p>The following sections detail:</p> <ul> <li>an overview of the model's rules and parameters.</li> <li>the properties of all entities stored in the model's state.</li> <li>the substeps that observe, simulate and modify the state for each agent.</li> <li>the code required to run the simulation using <code>agent-torch</code>.</li> <li>plotting the state's trajectory using <code>matplotlib</code>.</li> </ul>"},{"location":"tutorials/creating-a-model/#model-overview","title":"Model Overview","text":"<p>The following are configurable parameters of the model:</p> <ul> <li>a $n \\times m$ grid, with $p$ predators and $q$ prey to start with.</li> <li>grass can grown on any of the $n \\cdot m$ squares in the grid.</li> </ul> <p>The rules followed by the simulated interactions are configured as follows:</p> <ul> <li>predators can eat only prey, and prey can eat only grass.</li> <li>grass grows back once eaten after a certain number of steps.</li> <li>upon consuming food, the energy of the consumer increases.</li> <li>movement happens randomly, to any neighbouring square in the grid.</li> <li>each move reduces the energy of the entity by a fixed amount.</li> </ul> <p>These parameters and rules, along with the properties of the entities (detailed below) in the simulation are defined in a configuration file, and passed on to the model.</p>"},{"location":"tutorials/creating-a-model/#state-environment-agents-and-objects","title":"State: Environment, Agents, and Objects","text":"<p>The model's state consists of a list of properties of the simulated environment, and the agents and objects situated in that simulation. For this model, the:</p>"},{"location":"tutorials/creating-a-model/#environment","title":"Environment","text":"<p>The environment will have only one property: the size of the two-dimensional grid in which the predators and prey wander, defined like so:</p> <pre><code>environment:\n  bounds: (max_x, max_y) # tuple of integers\n</code></pre>"},{"location":"tutorials/creating-a-model/#agents","title":"Agents","text":"<p>This model has two agents: predator, and prey.</p>"},{"location":"tutorials/creating-a-model/#predator","title":"Predator","text":"<p>The predator agent is defined like so:</p> <pre><code>predator:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of randomly generated coordinates for all 40 predators.</p> <p>The <code>energy</code> property stores the current amount of energy possessed by the predator. Initially, this property is set to a random number between 30 and 100.</p> <p>The <code>stride_work</code> property is a static, but learnable property that stores the amount of energy to deduct from a predator for one step in any direction on the grid.</p>"},{"location":"tutorials/creating-a-model/#prey","title":"Prey","text":"<p>The prey agent is identical to the predator agent, and has one additional property: <code>nutritional_value</code>.</p> <pre><code>prey:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n  nutritional_value: float\n</code></pre> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#objects","title":"Objects","text":"<p>This model has only one agent: grass.</p>"},{"location":"tutorials/creating-a-model/#grass","title":"Grass","text":"<p>The grass entity is defined as follows:</p> <pre><code>grass:\n  coordinates: (x, y)\n  growth_stage: 0|1\n  growth_countdown: float\n  regrowth_time: float\n  nutritional_value: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of all 1600 coordinates.</p> <p>The <code>growth_stage</code> property stores the current growth stage of the grass: 0 means it is growing, and 1 means it is fully grown.</p> <p>The <code>growth_countdown</code> property stores the number of steps after which the grass becomes fully grown. The <code>regrowth_time</code> property is static and learnable, and stores the max value of the countdown property.</p> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#network","title":"Network","text":"<p>The model makes use of the adjacency matrix of a two-dimensional grid filled with predator and prey to simulate the movement of those entities.</p> <pre><code>network:\n  agent_agent:\n    grid: [predator, prey]\n</code></pre>"},{"location":"tutorials/creating-a-model/#substeps","title":"Substeps","text":"<p>Each substep is a <code>torch.nn.ModuleDict</code> that takes an input state, and produces an updated state as output. A substep consists of three phases:</p> <ol> <li>Observation (retrieving relevant information from the state)</li> <li>Policy/Action (deciding on the course of action as per the observations)</li> <li>Transition (randomizing and updating the state according to the action)</li> </ol> <p>This model consists of four substeps: <code>move</code>, <code>eat_grass</code>, <code>hunt_prey</code>, and <code>grow_grass</code>.</p> Helper functions <pre><code># define all the helper functions we need.\n\ndef get_neighbors(pos, adj_grid, bounds):\n  \"\"\"\n    Returns a list of neighbours for each position passed in the given\n    `pos` tensor, using the adjacency matrix passed in `adj_grid`.\n  \"\"\"\n  x, y = pos\n  max_x, max_y = bounds\n\n  # calculate the node number from the x, y coordinate.\n  # each item (i, j) in the adjacency matrix, if 1 depicts\n  # that i is connected to j and vice versa.\n  node = (max_y * x) + y\n  conn = adj_grid[node]\n\n  neighbors = []\n  for idx, cell in enumerate(conn):\n    # if connected, calculate the (x, y) coords of the other\n    # node and add it to the list of neighbors.\n    if cell == 1:\n      c = (int) (idx % max_y)\n      r = math.floor((idx - c) / max_y)\n\n      neighbors.append(\n        [torch.tensor(r), torch.tensor(c)]\n      )\n\n  return torch.tensor(neighbors)\n\n# define a function to retrieve the input required\ndef get_find_neighbors_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    adj_grid = get_var(state, input_variables['adj_grid'])\n    positions = get_var(state, input_variables['positions'])\n\n    return bounds, adj_grid, positions\n\ndef get_decide_movement_input(state, input_variables):\n    positions = get_var(state, input_variables['positions'])\n    energy = get_var(state, input_variables['energy'])\n\n    return positions, energy\n\ndef get_update_positions_input(state, input_variables):\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    prey_work = get_var(state, input_variables['prey_work'])\n    pred_work = get_var(state, input_variables['pred_work'])\n\n    return prey_energy, pred_energy, prey_work, pred_work\n\ndef get_find_eatable_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    positions = get_var(state, input_variables['positions'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n\n    return bounds, positions, grass_growth\n\ndef get_eat_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    energy = get_var(state, input_variables['energy'])\n    nutrition = get_var(state, input_variables['nutrition'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n    regrowth_time = get_var(state, input_variables['regrowth_time'])\n\n    return bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time\n\ndef get_find_targets_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n\n    return prey_pos, pred_pos\n\ndef get_hunt_prey_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    nutrition = get_var(state, input_variables['nutritional_value'])\n\n    return prey_pos, prey_energy, pred_pos, pred_energy, nutrition\n\ndef get_grow_grass_input(state, input_variables):\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n\n    return grass_growth, growth_countdown\n</code></pre>"},{"location":"tutorials/creating-a-model/#move","title":"Move","text":"<p>First, we observe the state, and find a list of neighboring positions for each of the predators/prey currently alive.</p> <pre><code>@Registry.register_substep(\"find_neighbors\", \"observation\")\nclass FindNeighbors(SubstepObservation):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state):\n    bounds, adj_grid, positions = get_find_neighbors_input(state, self.input_variables)\n\n    # for each agent (prey/predator), find the adjacent cells and pass\n    # them on to the policy class.\n    possible_neighbors = []\n    for pos in positions:\n      possible_neighbors.append(\n        get_neighbors(pos, adj_grid, bounds)\n      )\n\n    return { self.output_variables[0]: possible_neighbors }\n</code></pre> <p>Then, we decide the course of action: to move each entity to a random neighboring position, only if they have the energy to do so.</p> <pre><code>@Registry.register_substep(\"decide_movement\", \"policy\")\nclass DecideMovement(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    positions, energy = get_decide_movement_input(state, self.input_variables)\n    possible_neighbors = observations['possible_neighbors']\n\n    # randomly choose the next position of the agent. if the agent\n    # has non-positive energy, don't let it move.\n    next_positions = []\n    for idx, pos in enumerate(positions):\n      next_positions.append(\n        random.choice(possible_neighbors[idx]) if energy[idx] &gt; 0 else pos\n      )\n\n    return { self.output_variables[0]: torch.stack(next_positions, dim=0) }\n</code></pre> <p>Lastly, we update the state, with the new positions of the entities, and reduce the energy of each entity by the value of the <code>stride_work</code> learnable parameter.</p> <pre><code>@Registry.register_substep(\"update_positions\", \"transition\")\nclass UpdatePositions(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_energy, pred_energy, prey_work, pred_work = get_update_positions_input(state, self.input_variables)\n\n    # reduce the energy of the agent by the work required by them\n    # to take one step.\n    prey_energy = prey_energy + torch.full(prey_energy.shape, -1 * (prey_work.item()))\n    pred_energy = pred_energy + torch.full(pred_energy.shape, -1 * (pred_work.item()))\n\n    return {\n      self.output_variables[0]: action['prey']['next_positions'],\n      self.output_variables[1]: prey_energy,\n      self.output_variables[2]: action['predator']['next_positions'],\n      self.output_variables[3]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#eat","title":"Eat","text":"<p>First, decide which grass is fit to be consumed by the prey.</p> <pre><code>@Registry.register_substep(\"find_eatable_grass\", \"policy\")\nclass FindEatableGrass(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    bounds, positions, grass_growth = get_find_eatable_grass_input(state, self.input_variables)\n\n    # if the grass is fully grown, i.e., its growth_stage is equal to\n    # 1, then it can be consumed by prey.\n    eatable_grass_positions = []\n    max_x, max_y = bounds\n    for pos in positions:\n      x, y = pos\n      node = (max_y * x) + y\n      if grass_growth[node] == 1:\n        eatable_grass_positions.append(pos)\n\n    # pass on the consumable grass positions to the transition class.\n    return { self.output_variables[0]: eatable_grass_positions }\n</code></pre> <p>Then, simulate the consumption of the grass, and update the growth stage, growth countdown, and energies of the grass and prey respectively.</p> <pre><code>@Registry.register_substep(\"eat_grass\", \"transition\")\nclass EatGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time = get_eat_grass_input(state, self.input_variables)\n\n    # if no grass can be eaten, skip modifying the state.\n    if len(action['prey']['eatable_grass_positions']) &lt; 1:\n      return {}\n\n    eatable_grass_positions = torch.stack(action['prey']['eatable_grass_positions'], dim=0)\n    max_x, max_y = bounds\n    energy_mask = None\n    grass_mask, countdown_mask = torch.zeros(*grass_growth.shape), torch.zeros(*growth_countdown.shape)\n\n    # for each consumable grass, figure out if any prey agent is at\n    # that position. if yes, then mark that position in the mask as\n    # true. also, for all the grass that will be consumed, reset the\n    # growth stage.\n    for pos in eatable_grass_positions:\n      x, y = pos\n      node = (max_y * x) + y\n\n      # TODO: make sure dead prey cannot eat\n      e_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if energy_mask is None:\n        energy_mask = e_m\n      else:\n        energy_mask = e_m + energy_mask\n\n      grass_mask[node] = -1\n      countdown_mask[node] = regrowth_time - growth_countdown[node]\n\n    # energy + nutrition adds the `nutrition` tensor to all elements in\n    # the energy tensor. the (~energy_mask) ensures that the change is\n    # undone for those prey that did not consume grass.\n    energy = energy_mask*(energy + nutrition) + (~energy_mask)*energy\n\n    # these masks use simple addition to make changes to the original\n    # values of the tensors.\n    grass_growth = grass_growth + grass_mask\n    growth_countdown = growth_countdown + countdown_mask\n\n    return {\n      self.output_variables[0]: energy,\n      self.output_variables[1]: grass_growth,\n      self.output_variables[2]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#hunt","title":"Hunt","text":"<p>First, decide which prey are to be eaten.</p> <pre><code>@Registry.register_substep(\"find_targets\", \"policy\")\nclass FindTargets(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    prey_pos, pred_pos = get_find_targets_input(state, self.input_variables)\n\n    # if there are any prey at the same position as a predator,\n    # add them to the list of targets to kill.\n    target_positions = []\n    for pos in pred_pos:\n      if (pos == prey_pos).all(-1).any(-1) == True:\n        target_positions.append(pos)\n\n    # pass that list of targets to the transition class.\n    return { self.output_variables[0]: target_positions }\n</code></pre> <p>Then, update the energies of both the prey and the predator.</p> <pre><code>@Registry.register_substep(\"hunt_prey\", \"transition\")\nclass HuntPrey(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_pos, prey_energy, pred_pos, pred_energy, nutrition = get_hunt_prey_input(state, self.input_variables)\n\n    # if there are no targets, skip the state modifications.\n    if len(action['predator']['target_positions']) &lt; 1:\n      return {}\n\n    target_positions = torch.stack(action['predator']['target_positions'], dim=0)\n\n    # these are masks similars to the ones in `substeps/eat.py`.\n    prey_energy_mask = None\n    pred_energy_mask = None\n    for pos in target_positions:\n      pye_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if prey_energy_mask is None:\n        prey_energy_mask = pye_m\n      else:\n        prey_energy_mask = prey_energy_mask + pye_m\n\n      pde_m = (pos == pred_pos).all(dim=1).view(-1, 1)\n      if pred_energy_mask is None:\n        pred_energy_mask = pde_m\n      else:\n        pred_energy_mask = pred_energy_mask + pde_m\n\n    # any prey that is marked for death should be given zero energy.\n    prey_energy = prey_energy_mask*0 + (~prey_energy_mask)*prey_energy\n    # any predator that has hunted should be given additional energy.\n    pred_energy = pred_energy_mask*(pred_energy + nutrition) + (~pred_energy_mask)*pred_energy\n\n    return {\n      self.output_variables[0]: prey_energy,\n      self.output_variables[1]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#grow","title":"Grow","text":"<p>In this substep, we simply update the growth countdown of every grass object, and if the countdown has elapsed, we update the growth stage to <code>1</code>.</p> <pre><code>@Registry.register_substep(\"grow_grass\", \"transition\")\nclass GrowGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    grass_growth, growth_countdown = get_grow_grass_input(state, self.input_variables)\n\n    # reduce all countdowns by 1 unit of time.\n    growth_countdown_mask = torch.full(growth_countdown.shape, -1)\n    growth_countdown = growth_countdown + growth_countdown_mask\n\n    # if the countdown has reached zero, set the growth stage to 1,\n    # otherwise, keep it zero.\n    grass_growth_mask = (growth_countdown &lt;= 0).all(dim=1)\n    grass_growth = grass_growth_mask*(1) + (~grass_growth_mask)*(0)\n\n    return {\n      self.output_variables[0]: grass_growth.view(-1, 1),\n      self.output_variables[1]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#execution-configuration-registry-and-runner","title":"Execution: Configuration, Registry, and Runner","text":""},{"location":"tutorials/creating-a-model/#configuration","title":"Configuration","text":"<p>There are several parts to the configuration, written in a file traditionally called <code>config.yaml</code>. The following is a brief overview of all the major sections in the configuration file.</p> <pre><code># config.yaml\n# configuration for the predator-prey model.\n\nmetadata:\n  # device type, episode count, data files, etc.\n\nstate:\n  environment:\n    # variables/properties of the simulated enviroment.\n\n  agents:\n    # a list of agents in the simulation, and their properties.\n    # each property must be initialized by specifying a value\n    # or a generator function, and have a fixed tensor shape.\n\n  objects:\n    # a list of objects, similar to the agents list.\n\n  network:\n    # a list of interaction models for the simulation.\n    # could be a grid, or a directed graph, etc.\n\nsubsteps:\n  # a list of substeps\n  # each substep has a list of agents to run that substep for\n  # as well as the function, input and output variables for each\n  # part of that substep (observation, policy and transition)\n</code></pre> <p>The following is an example of defining a property in the configuration.</p> <pre><code>bounds:\n  name: 'Bounds'\n  learnable: false\n  shape: 2\n  dtype: 'int'\n  value:\n    - ${simulation_metadata.max_x} # you can refer to other parts of the config using\n    - ${simulation_metadata.max_y} # the template syntax, i.e., ${path.to.config.value}\n  initialization_function: null\n</code></pre> <p>Notice that to define one single property, we mentioned:</p> <ul> <li>the name of the property, here, <code>'bounds'</code>.</li> <li>whether or not the property is learnable, in this case, <code>false</code>.</li> <li>the shape of the tensor that stores the values, in this case, it is a   one-dimensional array of two elements: <code>(max_x, max_y)</code>.</li> <li>the value of the property, either by directly providing the value or by   providing a function that returns the value.</li> </ul> <p>The full configuration for the predator-prey model can be found here.</p> <pre><code># define helper functions used in the configuration\n\n@Registry.register_helper('map', 'network')\ndef map_network(params):\n  coordinates = (40.78264403323726, -73.96559413265355) # central park\n  distance = 550\n\n  graph = ox.graph_from_point(coordinates, dist=distance, simplify=True, network_type=\"walk\")\n  adjacency_matrix = nx.adjacency_matrix(graph).todense()\n\n  return graph, torch.tensor(adjacency_matrix)\n\n@Registry.register_helper('random_float', 'initialization')\ndef random_float(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random floating point\n    numbers in between and including the lower and upper limit.\n  \"\"\"\n\n  max = params['upper_limit'] + 1 # include max itself.\n  min = params['lower_limit']\n\n  # torch.rand returns a tensor of the given shape, filled with\n  # floating point numbers in the range (0, 1]. multiplying the\n  # tensor by max - min and adding the min value ensure it's\n  # within the given range.\n  tens = (max - min) * torch.rand(shape) + min\n\n  return tens\n\n@Registry.register_helper('random_int', 'initialization')\ndef random_int(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random integers in\n    between and including the lower and upper limit.\n  \"\"\"\n\n  max = math.floor(params['upper_limit'] + 1) # include max itself.\n  min = math.floor(params['lower_limit'])\n\n  # torch.randint returns the tensor we need.\n  tens = torch.randint(min, max, shape)\n\n  return tens\n</code></pre>"},{"location":"tutorials/creating-a-model/#registry-and-runner","title":"Registry and Runner","text":"<p>The code that executes the simulation uses the AgentTorch <code>Registry</code> and <code>Runner</code>, like so:</p> <pre><code>config = read_config('config-map.yaml')\nmetadata = config.get('simulation_metadata')\nnum_episodes = metadata.get('num_episodes')\nnum_steps_per_episode = metadata.get('num_steps_per_episode')\nnum_substeps_per_step = metadata.get('num_substeps_per_step')\n</code></pre> <p>The registry is stores all the classes and functions used by the model, and allows the runner to call them as needed when intializing the simulation and executing the substeps.</p> <pre><code>registry = Registry()\nregistry.register(read_from_file, 'read_from_file', 'initialization')\nregistry.register(grid_network, 'grid', key='network')\n</code></pre> <p>The runner intializes and executes the simulation for us. It also returns:</p> <ul> <li>a list of the learnable parameters, so we can run optimization functions on   them and use the optimized values for the next episode.</li> <li>the trajectory of the state so far, so we can visualize the state using   libraries like <code>matplotlib</code>.</li> </ul> <pre><code>runner = Runner(config, registry)\n</code></pre> <p> The source code for the visualizer used in the following block is given in the next section. </p> <pre><code>runner.init()\n\nfor episode in range(num_episodes):\n  runner.step(num_steps_per_episode)\n\n  final_states = list(filter(\n    lambda x: x['current_substep'] == str(num_substeps_per_step - 1),\n    runner.state_trajectory[-1]\n  ))\n  visualizer = Plot(metadata.get('max_x'), metadata.get('max_y'))\n  visualizer.plot(final_states)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"tutorials/creating-a-model/#visualization","title":"Visualization","text":"<p>You can plot the simulation in different ways. In this notebook, two such methods are demonstrated; the X-Y grid, and the OpenStreetMap plot.</p> <pre><code># display the gifs\n\nfrom IPython.display import HTML\n\nHTML(\"\"\"\n    &lt;table&gt;\n    &lt;tr&gt;&lt;td&gt;\n    &lt;video alt=\"grid\" autoplay&gt;\n        &lt;source src=\"../predator-prey.mp4\" type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n    &lt;/td&gt;&lt;td&gt;\n    &lt;img src=\"../predator-prey.gif\" alt=\"map\" /&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n\"\"\")\n</code></pre> <pre><code># render the map\n\nfrom IPython.display import display, clear_output\n\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plotter\nimport matplotlib.patches as patcher\nimport contextily as ctx\n\n%matplotlib inline\n\nclass Plot:\n  def __init__(self, max_x, max_y):\n    # intialize the scatterplot\n    self.figure, self.axes = None, None\n    self.prey_scatter, self.pred_scatter = None, None\n    self.max_x, self.max_y = max_x, max_y\n\n    plotter.xlim(0, max_x - 1)\n    plotter.ylim(0, max_y - 1)\n    self.i = 0\n\n  def update(self, state):\n    graph = state['network']['agent_agent']['predator_prey']['graph']\n    self.coords = [(node[1]['x'], node[1]['y']) for node in graph.nodes(data=True)]\n    self.coords.sort(key=lambda x: -(x[0] + x[1]))\n\n    self.figure, self.axes = ox.plot_graph(graph, edge_linewidth=0.3, edge_color='gray', show=False, close=False)\n    ctx.add_basemap(self.axes, crs=graph.graph['crs'], source=ctx.providers.OpenStreetMap.Mapnik)\n    self.axes.set_axis_off()\n\n    # get coordinates of all the entities to show.\n    prey = state['agents']['prey']\n    pred = state['agents']['predator']\n    grass = state['objects']['grass']\n\n    # agar energy &gt; 0 hai... toh zinda ho tum!\n    alive_prey = prey['coordinates'][torch.where(prey['energy'] &gt; 0)[0]]\n    alive_pred = pred['coordinates'][torch.where(pred['energy'] &gt; 0)[0]]\n    # show only fully grown grass, which can be eaten.\n    grown_grass = grass['coordinates'][torch.where(grass['growth_stage'] == 1)[0]]\n\n    alive_prey_x, alive_prey_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_prey\n    ]).T\n    alive_pred_x, alive_pred_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_pred\n    ]).T\n\n    # show prey in dark blue and predators in maroon.\n    self.axes.scatter(alive_prey_x, alive_prey_y, c='#0d52bd', marker='.')\n    self.axes.scatter(alive_pred_x, alive_pred_y, c='#8b0000', marker='.')\n\n    # increment the step count.\n    self.i += 1\n    # show the current step count, and the population counts.\n    self.axes.set_title('Predator-Prey Simulation #' + str(self.i), loc='left')\n    self.axes.legend(handles=[\n      patcher.Patch(color='#fc46aa', label=str(self.i) + ' step'),\n      patcher.Patch(color='#0d52bd', label=str(len(alive_prey)) + ' prey'),\n      patcher.Patch(color='#8b0000', label=str(len(alive_pred)) + ' predators'),\n      # patcher.Patch(color='#d1ffbd', label=str(len(grown_grass)) + ' grass')\n    ])\n\n    display(plotter.gcf())\n    clear_output(wait=True)\n    time.sleep(1)\n\n  def plot(self, states):\n    # plot each state, one-by-one\n    for state in states:\n      self.update(state)\n\n    clear_output(wait=True)\n</code></pre>"},{"location":"tutorials/creating-archetypes/","title":"Benchmark: Modeling with Archetypes","text":""},{"location":"tutorials/creating-archetypes/#archetype-tutorial","title":"Archetype Tutorial","text":""},{"location":"tutorials/creating-archetypes/#step-1-setup","title":"Step 1: Setup","text":"<p>First, let's set up our environment and import the necessary libraries:</p> <pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\nfrom agent_torch.core.llm.backend import LangchainLLM\nOPENAI_API_KEY = None\n</code></pre> <p>Setup : Covid Cases Data and Unemployment Rate</p> <pre><code>from utils import get_covid_cases_data\ncsv_path = '/models/covid/data/county_data.csv'\nmonthly_cases_kings = get_covid_cases_data(csv_path=csv_path,county_name='Kings County')\n\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-2-initialise-llm-instance","title":"Step 2: Initialise LLM Instance","text":"<p>We can use either of the Langchain and Dspy backends to initialise a LLM instance. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch.</p> <p>Let's see how we can use Langchain to initialise an LLM instance</p> <p>GPT 3.5 Turbo</p> <pre><code>agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\nllm_langchain_35 = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n)\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-3-define-an-archetype","title":"Step 3: Define an Archetype","text":"<pre><code># Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype_n_2 = Archetype(n_arch=2) \narchetype_n_12 = Archetype(n_arch=12)\n</code></pre> <p>Create an object of the Behavior class</p> <pre><code># Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age}, gender is {gender}, ethnicity is {ethnicity}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior_n_2 = Behavior(\n    archetype=archetype_n_2.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n    region=NYC\n)\nearning_behavior_n_12 = Behavior(\n    archetype=archetype_n_12.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n    region=NYC\n)\n</code></pre> <pre><code># Define arguments to be used for creating a query for the LLM Instance\nkwargs = {\n    \"month\": \"January\",\n    \"year\": \"2020\",\n    \"covid_cases\": 1200,\n    \"device\": \"cpu\",\n    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n    \"unemployment_rate\": 0.05,\n}\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-4-compare-performance-between-different-configurations-of-archetype","title":"Step 4: Compare performance between different Configurations of Archetype","text":"<pre><code>from utils import get_labor_data, get_labor_force_correlation\n\nlabor_force_df_n_2, observed_labor_force_n_2, correlation_n_2 = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_n_2, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nlabor_force_df_n_12, observed_labor_force_n_12, correlation_n_12 = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_n_12, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nprint(f\"Correlation with 2 Archetypes is {correlation_n_2} and 12 Archetypes is {correlation_n_12}\")\n</code></pre>"},{"location":"tutorials/differentiable-discrete-sampling/","title":"Differentiable Discrete Sampling using AgentTorch","text":""},{"location":"tutorials/differentiable-discrete-sampling/#introduction","title":"Introduction","text":"<p>Discrete sampling poses significant challenges in gradient-based optimization due to its non-differentiable nature, which prevents effective backpropagation. Operations like argmax disrupt gradient flow, leading to high-variance or biased gradient estimates. The Gumbel-Softmax technique addresses this by using a reparameterization trick that adds Gumbel noise to logits and applies a temperature-controlled softmax, enabling differentiable approximations of discrete samples. As the temperature approaches zero, the method produces near-discrete outputs while maintaining gradient flow, making it suitable for integrating discrete sampling into neural networks.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#rethinking-discrete-sampling","title":"Rethinking Discrete Sampling","text":"<p>It was assumed that Gumbel softmax solves this problem. However, Gumbel-Softmax has its own limitations. The temperature parameter introduces a bias-variance tradeoff: higher temperatures smooth gradients but deviate from true categorical distributions, while lower temperatures yield near-discrete samples with unstable gradients. Additionally, its continuous approximations may require straight-through estimators, which can introduce bias during backpropagation. These issues make Gumbel-Softmax less effective in tasks requiring precise distribution matching or structured outputs, highlighting the need for further improvements in discrete sampling techniques.</p> <p>So we introduce a new method for discrete sampling using the <code>agent_torch.core.distribution.Categorical</code> class. This class provides a differentiable approximation to discrete sampling, allowing for gradient-based optimization while maintaining the integrity of the categorical distribution.</p> <p>This estimator can simply be used as follows:</p> <pre><code>import torch\nfrom agent_torch.core.distributions import Categorical\n# Define the probabilities for each category\nprobs = torch.tensor([0.2, 0.5, 0.3], dtype=torch.float32)\n# Create a Categorical distribution\nsample = Categorical.apply(probs)\n# The sample will be a tensor containing the sampled category\nprint(sample)\n</code></pre> <p>Let's discuss more about this by seeing its application in various experiments.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-1-random-walk","title":"Experiment 1: Random Walk","text":"<p>Let's implement a 1D markovian random walk X0, X1, ...., Xn using the <code>agent_torch.core.distribution.Categorical</code> sampling method. The agent can move left or right with probabilites:</p> <ul> <li>Xn+1 = Xn + 1 with probability e^(-Xn/p)</li> <li>Xn+1 = Xn - 1 with probability 1 - e^(-Xn/p)</li> </ul> <p>First, lets import the important modules:</p> <pre><code>import torch\nimport math\nfrom agent_torch.core.distributions import Categorical\n</code></pre> <p>We are interested in studying the asymptotic behavior of the variance of our automatically derived gradient estimator, and so set p = n so that the transition function varies appreciably over the range of the walk for all n.</p> <p>Let's define the main function:</p> <pre><code>def random_walk_categorical(n, p, device):\n    x = 0.0  # initial state\n    path = [0.0]\n    for _ in range(n):\n        # Compute the probability of moving up.\n        q = math.exp(-x / p)\n        prob = torch.tensor([q, 1.0 - q], dtype=torch.float32, device=device).unsqueeze(0)  \n        # Sample an action using the custom Categorical function.\n        sample = Categorical.apply(prob)  \n        move = 1 if sample.item() == 0 else -1\n        # if at x==0, a downward move is overridden, since probability for going up is 1.\n        if x == 0 and move == -1:\n            move = 1\n        x += move\n        path.append(x)\n    return path\n</code></pre> <p>This random walk can be generated by:</p> <pre><code>n = 20  # A 20 step simulation\np = n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrandom_path = random_walk_categorical(n,p,device)\n\n# This random walk looks like [0,1,2,1,...]\n</code></pre> <p>Having seen how a random walk is implemented using AgentTorch, let's benchmark this against the Gumbel softmax method. The Gumbel softmax method is a differentiable approximation to the categorical distribution, allowing for gradient-based optimization. Let's discuss the experiment setup.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-setup","title":"Experiment Setup","text":"<p>This experiment focuses on the optimization of a parameter \u03b8 (theta) embedded within an exponential probability distribution function exp(-(x + \u03b8)/p), which governs the stochastic transition dynamics of our model. The primary objective is to calibrate \u03b8 such that the model's behavior closely approximates a baseline implementation, as measured by mean squared error (MSE).</p> <p>The methodology involves generating a substantial dataset comprising 1,000 trajectories, each consisting of 100 discrete time steps. This dataset is partitioned following standard machine learning protocols, with 70% allocated for parameter estimation (training) and 30% reserved for out-of-sample validation (testing).</p> <p>By systematically adjusting \u03b8, we aim to modulate the underlying probability distribution, thereby altering the likelihood of specific state transitions. This parameter optimization process seeks to minimize the discrepancy between the simulated trajectories and those produced by the baseline model. The efficacy of each candidate value for \u03b8 is quantitatively assessed via the MSE metric, which provides a rigorous measure of the deviation between the predicted and reference trajectories.</p> <p>This approach enables the fine-tuning of stochastic models to replicate observed phenomena with enhanced precision, with potential applications in various domains including statistical physics, financial modeling, and computational biology.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#results","title":"Results","text":"<p>The empirical findings demonstrate that the <code>agent_torch.core.distribution.Categorical</code> approach consistently exhibits superior performance metrics compared to the Gumbel-based method. Specifically, the <code>agent_torch.core.distribution.Categorical</code> method maintains consistently lower Wasserstein distance values across all experimental configurations, indicating better alignment between simulated and baseline distributions. Furthermore, the <code>agent_torch.core.distribution.Categorical</code> approach effectively preserves the variance ratio at approximately unity, which substantiates that the generated trajectories maintain distributional characteristics highly comparable to those of the baseline.</p> <p>Although the parameter convergence behavior varies across different initialization points, particularly for initial values of 10.0 and 0.0, the distributional properties of the Categorical method's outputs remain demonstrably superior to those produced by the Gumbel approach. This superiority is quantitatively verified through both lower Wasserstein distance measurements and reduced mean squared error metrics, which collectively indicate that the <code>agent_torch.core.distribution.Categorical</code> method generates distributions with greater fidelity to the baseline distribution regardless of initialization conditions. These results suggest that the <code>agent_torch.core.distribution.Categorical</code> approach provides a more robust framework for distribution matching in this experimental context, maintaining consistent performance advantages across varied experimental configurations.</p> <p>These results will further become clear when we plot these random walks. It can clearly be infered that the Gumbel method starts diverging from the baseline and performs poorly on the test dataset.</p> <p> </p> <p>First, we run the experiment for 100 time-steps and calibrate theta values for both methods. Among the methods, <code>agent_torch.core.distribution.Categorical</code> stays relatively close to the baseline, while the Gumbel-based approach begins to drift early and deviates substantially in the testing region. This suggests that the Categorical method generalizes better across regions and is more stable under extended evaluation.</p> <p>Second, we extend the experiment to 1000 steps to examine long-term behavior. Over this longer horizon, the difference becomes even more pronounced. Gumbel's trajectory continues to diverge and accumulates a large positional error, confirming its poor generalization performance. In contrast, <code>agent_torch.core.distribution.Categorical</code> remains much more aligned with the baseline throughout.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-2-neural-relational-inference","title":"Experiment 2: Neural Relational Inference","text":"<p>The neural relational inference experiment is designed to infer and model latent interactions among entities in a dynamic system. In this experiment, a graph-based neural architecture is employed in which a factor graph CNN encoder extracts relational features from observed data, while a learned recurrent interaction net decoder predicts future states by modeling interactions between nodes (or atoms). The goal is to simultaneously learn the underlying relations and use these learned interactions to improve prediction accuracy and interpretability of the system\u2019s dynamics.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-setup_1","title":"Experiment Setup","text":"<p>The NRI experiment specifically focuses on learning to infer interactions in physical systems without supervision. The model is structured as a variational auto-encoder where the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. The researchers conducted experiments on simulated physical systems including springs and charged particles. The model is evaluated on its ability to recover ground-truth interactions in these simulated environments, as well as its capacity to find interpretable structure and predict complex dynamics in real-world data such as motion capture and sports tracking data</p> <p>Initially, the experiment employed a Gumbel-Softmax approach for discrete sampling. In this setup, the addition of Gumbel noise and a temperature-controlled softmax allowed for differentiable approximations of categorical samples. However, the inherent bias-variance tradeoff\u2014where higher temperatures yield smoother but less discrete gradients, and lower temperatures produce near-discrete but unstable gradients\u2014limits the method's effectiveness. While the negative log-likelihood decreases over epochs, the KL divergence remains relatively low, suggesting insufficient regularization of the discrete structure .</p> <p>Recognizing these limitations, the experiment was repeated using our <code>agent_torch.core.distribution.Categorical</code> class. This new estimator directly provides a differentiable approximation for discrete sampling, bypassing some of the drawbacks inherent in the Gumbel-Softmax method. Notably, by more tightly coupling the sampling process to the categorical distribution, the estimator mitigates the bias-variance issue and improves gradient stability during training.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#results_1","title":"Results","text":"<p>The training logs for the categorical estimator experiment reveal several improvements:</p> <ul> <li>Stable KL Divergence: The KL divergence values remained consistent from early epochs into convergence. This higher and stable KL value suggests that the model is enforcing a stronger regularization on the inferred discrete relations, leading to a more consistent latent structure.</li> <li>Lower Negative Log-Likelihood: While both methods converge to low nll_train values as training proceeds, the categorical estimator maintains comparably low loss values alongside improved training accuracy. </li> <li>Improved Predictive Accuracy: The accuracy trends in the logs show that the categorical estimator experiment reaches and sustains higher accuracy levels. The results point to a model that not only fits the data better but also generalizes more effectively\u2014an essential trait when dealing with structured, relational data.</li> </ul>"},{"location":"tutorials/differentiable-discrete-sampling/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrated how to implement and use differentiable discrete sampling operations using AgentTorch.</p>"},{"location":"tutorials/integrating-with-beckn/","title":"AgentTorch-Beckn Solar Model","text":""},{"location":"tutorials/integrating-with-beckn/#overview","title":"Overview","text":"<p>AgentTorch is a differentiable learning framework that enables you to run simulations with over millions of autonomous agents. Beckn is a protocol that enables the creation of open, peer-to-peer decentralized networks for pan-sector economic transactions.</p> <p>This model integrates Beckn with AgentTorch, to simulate a solar energy network in which households in a locality can decide to either buy solar panels and act as providers of solar energy, or decide to use the energy provided by other households instead of installing solar panels themselves.</p> <p></p> <p>A visualization of increase in net solar energy used per street.</p>"},{"location":"tutorials/integrating-with-beckn/#mapping-beckn-protocol-to-agenttorch","title":"Mapping Beckn Protocol to AgentTorch","text":""},{"location":"tutorials/integrating-with-beckn/#1-network","title":"1. Network","text":"<p>The participants in the Beckn network (providers, customers and gateways) are considered agents that interact with each other.</p>"},{"location":"tutorials/integrating-with-beckn/#2-operations","title":"2. Operations","text":"<p>The following operations are simulated as substeps:</p>"},{"location":"tutorials/integrating-with-beckn/#1-a-customer-will-search-and-select-a-provider","title":"1. a customer will <code>search</code> and <code>select</code> a provider","text":"<ul> <li>the customer selects the closest provider with the least price</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#2-the-customer-will-order-from-the-provider","title":"2. the customer will <code>order</code> from the provider","text":"<ul> <li>the customer orders basis their monthly energy demand</li> <li>the provider only confirms the order if it has the capacity to</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#3-the-provider-will-fulfill-the-order","title":"3. the provider will <code>fulfill</code> the order","text":"<ul> <li>the provider's capacity is reduced for the given step (~= 30 real days)</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#4-the-customer-will-pay-for-the-work-done","title":"4. the customer will <code>pay</code> for the work done","text":"<ul> <li>the provider's revenue is incremented, while the customer's wallet is deducted the same   amount.</li> <li>the amount to be paid is determined by the provider's price, multiplied by the amount of   energy supplied.</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#5-the-provider-will-restock-their-solar-energy","title":"5. the provider will <code>restock</code> their solar energy","text":"<ul> <li>the amount of energy replenished SHOULD BE (TODO) dependent on the season as well as the   weather.</li> </ul> <p>Each of the substeps' code (apart from #5) is taken as-is from the AgentTorch Beckn integration.</p> <p>Note that while Beckn's API calls are asynchronous, the simulation assumes they are synchronous for simplicity.</p>"},{"location":"tutorials/integrating-with-beckn/#3-data","title":"3. Data","text":"<p>The data for this example model is currently sourced from various websites, mostly from data.boston.gov. However, the data should actually come from the Beckn Protocol's implementation of a solar network.</p>"},{"location":"tutorials/integrating-with-beckn/#running-the-model","title":"Running the Model","text":"<p>To run the model, clone the github repository first:</p> <pre><code># git clone --depth 1 --branch solar https://github.com/AgentTorch/agent-torch-beckn solar-netowkr\n</code></pre> <p>Then, setup a virtual environment and install all dependencies:</p> <pre><code># cd solar-network/\n# python -m venv .venv/bin/activate\n# . .venv/bin/activate\n# pip install -r requirements.txt\n</code></pre> <p>Once that is done, you can edit the configuration (<code>config.yaml</code>), and change the data used in the simulation by editing the simulation's data files (<code>data/simulator/{agent}/{property}.csv</code>).</p> <p>Then, open Jupyter Lab and open the <code>main.ipynb</code> notebook, and run all the cells.</p> <pre><code># pip install jupyterlab\n# jupyter lab\n</code></pre>"},{"location":"tutorials/integrating-with-beckn/#todos","title":"Todos","text":"<ul> <li>Add more visualizations (plots/graphs/heatmaps/etc.)</li> <li>Improve the data used for the simulation, reduce the number of random values.</li> <li>Add more detailed logic to the substeps, i.e., seasonal fluctuation in energy generation   and prices.</li> <li>Include and run a sample beckn instance to pull fake data from.</li> </ul>"},{"location":"tutorials/processing-a-population/","title":"Tutorial: Generating Base Population and Household Data","text":"<p>This tutorial will guide you through the process of generating base population and household data for a specified region using census data. We\u2019ll use a <code>CensusDataLoader</code> class to handle the data processing and generation.</p>"},{"location":"tutorials/processing-a-population/#before-starting","title":"Before Starting","text":"<p>Make sure your <code>population data</code> and <code>household data</code> are in the prescribed format. Names of the column need to be same as shown in the excerpts.</p> <p>Lets see a snapshot of the data</p> <p><code>Population Data</code> is a dictionary containing two pandas DataFrames: '<code>age_gender</code>' and '<code>ethnicity</code>'. Each DataFrame provides demographic information for different areas and regions.</p> <p>The <code>age_gender</code> DataFrame provides a comprehensive breakdown of population data, categorized by area, gender, and age group.</p>"},{"location":"tutorials/processing-a-population/#columns-description","title":"Columns Description","text":"<ul> <li><code>area</code>: Serves as a unique identifier for each geographical area, represented   by a string (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>).</li> <li><code>gender</code>: Indicates the gender of the population segment, with possible values   being <code>'female'</code> or <code>'male'</code>.</li> <li><code>age</code>: Specifies the age group of the population segment, using a string   format such as <code>'20t29'</code> for ages 20 to 29, and <code>'U19'</code> for those under 19   years of age.</li> <li><code>count</code>: Represents the total number of individuals within the specified   gender and age group for a given area.</li> <li><code>region</code>: A two-letter code that identifies the broader region encompassing   the area (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry","title":"Example Entry","text":"<p>Here is a sample of the data structure within the <code>age_gender</code> DataFrame:</p> area gender age count region BK0101 female 20t29 3396 BK BK0101 male 20t29 3327 BK <p>This example entry demonstrates the DataFrame's layout and the type of demographic data it contains, highlighting its utility for detailed population studies by age and gender.</p> <p>The <code>ethnicity</code> DataFrame is structured to provide detailed population data, segmented by both geographical areas and ethnic groups.</p>"},{"location":"tutorials/processing-a-population/#columns-description_1","title":"Columns Description","text":"<ul> <li><code>area</code>: A unique identifier assigned to each area, formatted as a string   (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>). This identifier helps in pinpointing specific   locations within the dataset.</li> <li><code>ethnicity</code>: Represents the ethnic group of the population in the specified   area.</li> <li><code>count</code>: Indicates the number of individuals belonging to the specified ethnic   group within the area. This is an integer value representing the population   count.</li> <li><code>region</code>: A two-letter code that signifies the broader region that the area   belongs to (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry_1","title":"Example Entry","text":"<p>Below is an example of how the data is presented within the DataFrame:</p> area ethnicity count region BK0101 asian 1464 BK BK0101 black 937 BK <p>This example illustrates the structure and type of data contained within the <code>ethnicity</code> DataFrame, showcasing its potential for detailed demographic studies.</p> <p><code>Household Data</code> contains the following columns:</p> <ul> <li><code>area</code>: Represents a unique identifier for each area.</li> <li><code>people_num</code>: The total number of people within the area.</li> <li><code>children_num</code>: The number of children in the area.</li> <li><code>household_num</code>: The total number of households.</li> <li><code>family_households</code>: Indicates the number of households identified as family   households, highlighting family-based living arrangements.</li> <li><code>nonfamily_households</code>: Represents the number of households that do not fall   under the family households category, including single occupancy and unrelated   individuals living together.</li> <li><code>average_household_size</code>: The average number of individuals per household.</li> </ul> <p>Below is a sample excerpt:</p> area people_num children_num household_num family_households nonfamily_households average_household_size 100100 104 56 418 1 0 2.488038 100200 132 73 549 1 0 2.404372 100300 5 0 10 0 1 5.000000 <p>Now that we have verified our input, we can proceed to next steps!</p>"},{"location":"tutorials/processing-a-population/#step-1-set-up-file-paths","title":"Step 1: Set Up File Paths","text":"<p>First, we need to specify the paths to our data files.</p> <p>Make sure to replace the placeholder paths with the actual paths to your data files.</p> <pre><code># Path to the population data file. Update with the actual file path.\nPOPULATION_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/population.pkl\"\n\n# Path to the household data file. Update with the actual file path.\nHOUSEHOLD_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/household.pkl\"\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-2-define-age-group-mapping","title":"Step 2: Define Age Group Mapping","text":"<p>We\u2019ll define a mapping for age groups to categorize adults and children in the household data:</p> <pre><code>AGE_GROUP_MAPPING = {\n    \"adult_list\": [\"20t29\", \"30t39\", \"40t49\", \"50t64\", \"65A\"],  # Age ranges for adults\n    \"children_list\": [\"U19\"],  # Age range for children\n}\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-3-load-data","title":"Step 3: Load Data","text":"<p>Now, let\u2019s load the population and household data:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Load household data\nHOUSEHOLD_DATA = pd.read_pickle(HOUSEHOLD_DATA_PATH)\n\n# Load population data\nBASE_POPULATION_DATA = pd.read_pickle(POPULATION_DATA_PATH)\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-4-set-up-additional-parameters","title":"Step 4: Set Up Additional Parameters","text":"<p>We\u2019ll set up some additional parameters that might be needed for data processing. These are not essential for generating population, but still good to know if you decide to use them in future.</p> <pre><code># Placeholder for area selection criteria, if any. Update or use as needed.\n# Example: area_selector = [\"area1\", \"area2\"]\n# This will be used to filter the population data to only include the selected areas.\narea_selector = None\n\n# Placeholder for geographic mapping data, if any. Update or use as needed.\ngeo_mapping = None\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-5-initialize-the-census-data-loader","title":"Step 5: Initialize the Census Data Loader","text":"<p>Create an instance of the <code>CensusDataLoader</code> class:</p> <pre><code>from agent_torch.data.census.census_loader import CensusDataLoader\n\ncensus_data_loader = CensusDataLoader(n_cpu=8, use_parallel=True)\n</code></pre> <p>This initializes the loader with 8 CPUs and enables parallel processing for faster data generation.</p>"},{"location":"tutorials/processing-a-population/#step-6-generate-base-population-data","title":"Step 6: Generate Base Population Data","text":"<p>Generate the base population data for a specified region:</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n)\n</code></pre> <p>This will create a base population of 100 individuals for the \u201castoria\u201d region. The generated data will be exported to a folder named \u201castoria\u201d under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#overview-of-the-generated-base-population-data","title":"Overview of the Generated Base Population Data","text":"<p>Each row corresponds to attributes of individual residing in the specified region while generating the population.</p> area age gender ethnicity region BK0101 20t29 female black BK BK0101 20t29 female hispanic BK ... ... ... ... ... BK0101 U19 male asian SI BK0101 U19 female white SI BK0101 U19 male asian SI"},{"location":"tutorials/processing-a-population/#step-7-generate-household-data","title":"Step 7: Generate Household Data","text":"<p>Finally, generate the household data for the specified region:</p> <pre><code>census_data_loader.generate_household(\n    household_data=HOUSEHOLD_DATA,  # The loaded household data\n    household_mapping=AGE_GROUP_MAPPING,  # Mapping of age groups for household composition\n    region=\"astoria\"  # The target region for generating households\n)\n</code></pre> <p>This will create household data for the \u201castoria\u201d region based on the previously generated base population. The generated data will be exported to the same \u201castoria\u201d folder under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#bonus-generate-population-data-of-specific-size","title":"Bonus: Generate Population Data of Specific Size","text":"<p>For quick experimentation, this may come in handy.</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n    num_individuals = 100 # Saves data for first 100 individuals, from the generated population\n)\n</code></pre>"},{"location":"tutorials/processing-a-population/#bonus-export-population-data","title":"Bonus: Export Population Data","text":"<p>If you have already generated your synthetic population, you just need to export it to \"populations\" folder under the desired \"region\", in order for you to use it with AgentTorch.</p> <pre><code>POPULATION_DATA_PATH = \"/population_data.pickle\"  # Replace with actual path\ncensus_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\")\n</code></pre> <p>In case you want to export data for only few individuals</p> <pre><code>census_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\",num_individuals = 100)\n</code></pre>"},{"location":"tutorials/processing-a-population/#conclusion","title":"Conclusion","text":"<p>You have now successfully generated both base population and household data for the <code>\u201castoria\u201d</code> region. The generated data can be found in the <code>\u201cpopulations/astoria\u201d</code> folder. You can modify the region name, population size, and other parameters to generate data for different scenarios.</p>"},{"location":"tutorials/simulation-and-calibration/","title":"Creating and Calibrating a Simulation","text":"<p>This tutorial will guide you through the process of creating a simulation instance and integrating it with calibration logic using AgentTorch. We'll cover:</p> <ol> <li>Setting up a basic simulation</li> <li>Configuring simulation parameters</li> <li>Running the simulation</li> <li>Integrating with calibration</li> <li>Advanced parameter tuning</li> </ol>"},{"location":"tutorials/simulation-and-calibration/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, make sure you have:</p> <ul> <li>AgentTorch installed (<code>pip install agent-torch</code>)</li> <li>Basic understanding of Python and PyTorch</li> <li>Your model and population data ready</li> </ul>"},{"location":"tutorials/simulation-and-calibration/#basic-simulation-setup","title":"Basic Simulation Setup","text":"<p>First, let's create a basic simulation instance. Here's a minimal example:</p> <pre><code>from agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\nfrom agent_torch.models import covid  # Example model\nfrom agent_torch.populations import sample2  # Example population\n\ndef setup_simulation(model, population):\n    # Create a population loader\n    loader = LoadPopulation(population)\n\n    # Initialize the simulation executor\n    simulation = Executor(model=model, pop_loader=loader)\n\n    # Get the runner instance\n    runner = simulation.runner\n\n    # Initialize the simulation\n    runner.init()\n\n    return runner\n\n# Create the simulation instance\nrunner = setup_simulation(covid, sample2)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#configuring-parameters","title":"Configuring Parameters","text":"<p>You can configure simulation parameters in two ways:</p> <ol> <li>During initialization:</li> </ol> <pre><code>simulation_config = {\n    'simulation_metadata': {\n        'num_steps_per_episode': 100,\n        'num_episodes': 1\n    }\n}\nrunner = setup_simulation(covid, sample2, config=simulation_config)\n</code></pre> <ol> <li>After initialization using the parameter API:</li> </ol> <pre><code>def set_parameter(runner, param_path, new_value):\n    \"\"\"\n    param_path: String path to the parameter (e.g., 'initializer.transition_function.0.new_transmission.learnable_args.R0')\n    new_value: New tensor value for the parameter\n    \"\"\"\n    params_dict = {param_path: new_value}\n    runner._set_parameters(params_dict)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#running-the-simulation","title":"Running the Simulation","text":"<p>To run the simulation:</p> <pre><code>def run_simulation(runner):\n    # Get simulation parameters\n    num_steps = runner.config['simulation_metadata']['num_steps_per_episode']\n\n    # Run simulation steps\n    runner.step(num_steps)\n\n    # Get final trajectory\n    final_trajectory = runner.state_trajectory[-1][-1]\n\n    return final_trajectory\n\n# Run simulation and get results\nresults = run_simulation(runner)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#integrating-with-calibration","title":"Integrating with Calibration","text":"<p>For calibration, we need to: 1. Define parameters to calibrate 2. Create a loss function 3. Set up optimization</p> <p>Here's how to do it:</p> <pre><code>import torch\nimport torch.optim as optim\n\ndef setup_calibration(runner):\n    # Get learnable parameters\n    learn_params = [(name, params) for (name, params) in runner.named_parameters()]\n\n    # Create optimizer\n    optimizer = optim.Adam(runner.parameters(), lr=0.01)\n\n    return optimizer\n\ndef calibration_step(runner, optimizer, target_data):\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Run simulation\n    trajectory = run_simulation(runner)\n\n    # Calculate loss (example using infected counts)\n    preds = trajectory['environment']['daily_infected']\n    loss = torch.nn.functional.mse_loss(preds, target_data)\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return loss.item()\n\n# Setup calibration\noptimizer = setup_calibration(runner)\n\n# Run calibration loop\nfor epoch in range(100):\n    loss = calibration_step(runner, optimizer, target_data)\n    print(f\"Epoch {epoch}, Loss: {loss}\")\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#advanced-parameter-tuning","title":"Advanced Parameter Tuning","text":"<p>For more fine-grained control over parameters:</p> <pre><code>def get_parameter(runner, param_path):\n    \"\"\"Get current parameter value\"\"\"\n    tensor_func = map_and_replace_tensor(param_path)\n    return tensor_func(runner)\n\ndef update_parameter(runner, param_path, new_value):\n    \"\"\"Update specific parameter with gradient tracking\"\"\"\n    assert isinstance(new_value, torch.Tensor)\n    assert new_value.requires_grad\n    set_parameter(runner, param_path, new_value)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always validate parameter changes:    <code>python    def validate_parameter(runner, param_path, new_value):        current_value = get_parameter(runner, param_path)        assert new_value.shape == current_value.shape, \"Shape mismatch\"        assert new_value.requires_grad == current_value.requires_grad, \"Gradient requirement mismatch\"</code></p> </li> <li> <p>Save and load calibrated parameters:    ```python    def save_parameters(runner, filepath):        torch.save(runner.state_dict(), filepath)</p> </li> </ol> <p>def load_parameters(runner, filepath):        runner.load_state_dict(torch.load(filepath))    ```</p>"},{"location":"tutorials/simulation-and-calibration/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Gradient Issues: If you encounter gradient-related errors, ensure all parameters that need gradients have <code>requires_grad=True</code>.</p> </li> <li> <p>Memory Management: For large simulations, consider using:    <code>python    def clear_memory(runner):        runner.state_trajectory = []  # Clear trajectory history        torch.cuda.empty_cache()  # If using GPU</code></p> </li> <li> <p>Parameter Bounds: Implement parameter constraints:    <code>python    def constrain_parameters(runner, param_path, min_val, max_val):        value = get_parameter(runner, param_path)        constrained_value = torch.clamp(value, min_val, max_val)        set_parameter(runner, param_path, constrained_value)</code></p> </li> </ol>"},{"location":"tutorials/simulation-and-calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Explore more advanced calibration techniques in the Calibration Advanced Guide</li> <li>Learn about analyzing simulation results in the Simulation Analysis Tutorial</li> <li>Understand how to integrate custom models in the Custom Models Guide </li> </ul>"}]}